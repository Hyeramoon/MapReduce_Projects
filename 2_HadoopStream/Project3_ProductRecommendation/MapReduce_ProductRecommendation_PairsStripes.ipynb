{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/**********************************************************************************************\n",
       "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
       "https://github.com/mathjax/MathJax/issues/1300\n",
       "A quick hack to fix this based on stackoverflow discussions: \n",
       "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
       "**********************************************************************************************/\n",
       "\n",
       "$('.math>span').css(\"border-left-color\",\"transparent\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "/**********************************************************************************************\n",
    "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
    "https://github.com/mathjax/MathJax/issues/1300\n",
    "A quick hack to fix this based on stackoverflow discussions: \n",
    "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
    "**********************************************************************************************/\n",
    "\n",
    "$('.math>span').css(\"border-left-color\",\"transparent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "\n",
    "\n",
    "## Project 3\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  Hyera Moon   \n",
    "__Week:__   3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a name=\"TOC\"></a> \n",
    "\n",
    "1.  [HW Intructions](#1)   \n",
    "2.  [HW Problems](#2)   \n",
    "    3.1.   [HW3.1](#3.1)     \n",
    "    3.2.   [HW3.2](#3.2)   \n",
    "    3.3  [HW3.3](#3.3)   \n",
    "    3.4  [HW3.4](#3.4)   \n",
    "    3.5  [HW3.5](#3.5)   \n",
    "    3.6  [HW3.6](#3.6)   \n",
    "    3.7  [HW3.7](#3.7)   \n",
    "    3.8  [HW3.8](#3.8)   \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "# 1 Instructions\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## HW3.0.\n",
    "1. How do you merge  two sorted  lists/arrays of records of the form [key, value]?\n",
    "1. Where is this  used in Hadoop MapReduce? [Hint within the shuffle]\n",
    "1. What is  a combiner function in the context of Hadoop? \n",
    "1. Give an example where it can be used and justify why it should be used in the context of this problem.\n",
    "1. What is the Hadoop shuffle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) To merge two sorted lists/arrays of records of the form [key,value], we need 3 arrays: the two sorted array and an empty temporary array. Then we define 3 references (pointers) at the front of each array and move the pointers by picking the smallest key of the 2 sorted arrays and move that key with the value to the temporary array, incrementing the corresponding indices.  \n",
    "\n",
    "\n",
    "2) This type of merge is used:  \n",
    "- on the map side: when merging spills in a single, partitioned file  \n",
    "- on the reduce side: to merge map outputs that have been copied over to the reducer  \n",
    "    \n",
    "    \n",
    "3) A combiner is a mapper side function that allows us to combine records from mapper outputs that have the same key. They can be thought as \"mini-reducers\" that run in memory after the map phase. Thus, it alleviates the network traffic in the shuffling phase.  \n",
    "\n",
    "\n",
    "4) An example where combiner can be used is word count of a large set of text documents. In this case, the outputs of the map task (each word in the document with a count of 1) will be very large, that is, a lot of data to transfer across the network from the mapper to the reducer. A combiner will help to aggregate the map outputs with the same key and thus will result in fewer records (e.g. 2 records with same key (word1, 1) and (word1, 1) will be combined into one single record (word1, 2)) and reduce the amount of data transfer across the network as inputs to the reducer.  \n",
    "\n",
    "\n",
    "5) Hadoop shuffle is a contract between the mapper and reducer: it is the process by which the system performs the sort and transfer of the map outputs from mappers to reducers as inputs. It can be summarized by \"partition, sort, combine\" (in memory, on disk). The partition, sort and combine is required so that the records with same key are in the same and the inputs to reducers are sorted by key.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.1\"></a>\n",
    "## HW3.1 consumer complaints dataset: Use Counters to do EDA (exploratory data analysis and to monitor progress)\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "Use the Consumer Complaints  Dataset provide here to complete this question:\n",
    "\n",
    "\n",
    "     https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
    "\n",
    "The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "Here’s is the first few lines of the  of the Consumer Complaints  Dataset:\n",
    "```\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
    "1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,\n",
    "1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "```\n",
    "User-defined Counters\n",
    "\n",
    "Now, let’s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 48.5M  100 48.5M    0     0  6132k      0  0:00:08  0:00:08 --:--:-- 6962k\n"
     ]
    }
   ],
   "source": [
    "# First download data from dropbox and create csv file\n",
    "!curl 'https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0#' -L -o ccd.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HDFS directories\n",
    "!hdfs dfs -mkdir -p hw3/complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put the data into HDFS\n",
    "!hdfs dfs -put ccd.csv hw3/complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing complaintCountsMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile complaintCountsMapper.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW31MAPPER\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    complaint_ID, product, other_info = line.split(\",\", 2)\n",
    "    if \"debt\" in product.lower():    \n",
    "        sys.stderr.write(\"reporter:counter:EDA Counters,Debt,1\\n\") \n",
    "        print '%s\\t%s' % (\"debt\", 1)\n",
    "    elif \"mortgage\" in product.lower():    \n",
    "        sys.stderr.write(\"reporter:counter:EDA Counters,Mortgage,1\\n\") \n",
    "        print '%s\\t%s' % (\"mortgage\", 1)\n",
    "    else:   \n",
    "        sys.stderr.write(\"reporter:counter:EDA Counters,Other,1\\n\") \n",
    "        print '%s\\t%s' % (\"other\", 1)\n",
    "\n",
    "# END STUDENT CODE HW31MAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting complaintCountsReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile complaintCountsReducer.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW31REDUCER\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    product, count = line.split('\\t', 1)\n",
    "\n",
    "    if current_word == product:\n",
    "        current_count += int(count)\n",
    "    else:\n",
    "        if current_word:\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = int(count)\n",
    "        current_word = product\n",
    "\n",
    "# Output the last word if needed\n",
    "if current_word == product:\n",
    "    print '%s\\t%s' % (current_word, current_count)\n",
    "\n",
    "# END STUDENT CODE HW31REDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x complaintCountsMapper.py\n",
    "!chmod a+x complaintCountsReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted HW3.1/results\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob7112962568325066335.jar tmpDir=null\n",
      "17/01/28 13:59:40 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/28 13:59:40 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/28 13:59:41 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/28 13:59:41 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/28 13:59:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485626579957_0003\n",
      "17/01/28 13:59:41 INFO impl.YarnClientImpl: Submitted application application_1485626579957_0003\n",
      "17/01/28 13:59:41 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1485626579957_0003/\n",
      "17/01/28 13:59:41 INFO mapreduce.Job: Running job: job_1485626579957_0003\n",
      "17/01/28 13:59:48 INFO mapreduce.Job: Job job_1485626579957_0003 running in uber mode : false\n",
      "17/01/28 13:59:48 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/28 13:59:58 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/01/28 14:00:00 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/28 14:00:06 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/28 14:00:07 INFO mapreduce.Job: Job job_1485626579957_0003 completed successfully\n",
      "17/01/28 14:00:07 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3462020\n",
      "\t\tFILE: Number of bytes written=7284243\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910814\n",
      "\t\tHDFS: Number of bytes written=40\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17327\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5071\n",
      "\t\tTotal time spent by all map tasks (ms)=17327\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5071\n",
      "\t\tTotal vcore-seconds taken by all map tasks=17327\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5071\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=17742848\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5192704\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312913\n",
      "\t\tMap output bytes=2836188\n",
      "\t\tMap output materialized bytes=3462026\n",
      "\t\tInput split bytes=232\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=3462026\n",
      "\t\tReduce input records=312913\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=625826\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=252\n",
      "\t\tCPU time spent (ms)=7900\n",
      "\t\tPhysical memory (bytes) snapshot=858660864\n",
      "\t\tVirtual memory (bytes) snapshot=4677038080\n",
      "\t\tTotal committed heap usage (bytes)=737673216\n",
      "\tEDA Counters\n",
      "\t\tDebt=44372\n",
      "\t\tMortgage=125752\n",
      "\t\tOther=142789\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=40\n",
      "17/01/28 14:00:07 INFO streaming.StreamJob: Output directory: HW3.1/results\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW31HADOOP\n",
    "\n",
    "!hdfs dfs -rm -r HW3.1/results # Comment only when running first time\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "  -files complaintCountsMapper.py,complaintCountsReducer.py \\\n",
    "  -mapper complaintCountsMapper.py \\\n",
    "  -reducer complaintCountsReducer.py \\\n",
    "  -input hw3/complaints/ccd.csv \\\n",
    "  -output HW3.1/results  \\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "# END STUDENT CODE HW31HADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<H1> Mapper Counters - Job Tracker UI Screenshot </H1>\n",
       "<img src=\"mapperCounters.png\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<H1> Mapper Counters - Job Tracker UI Screenshot </H1>\n",
    "<img src=\"mapperCounters.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.2\"></a>\n",
    "### HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters\n",
    "\n",
    "For this brief study the Input file will be one record (the next line only):    \n",
    "`foo foo quux labs foo bar quux`\n",
    "\n",
    "\n",
    "__3.2.A__     \n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many times the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain.\n",
    "\n",
    "__3.2.B__   \n",
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "\n",
    "__3.2.C__     \n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "\n",
    "Using a single reducer: \n",
    "- What are the top 50 most frequent terms in your word count analysis?    \n",
    "- Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order.    \n",
    "- Present bottom 10 tokens (least frequent items). \n",
    "\n",
    "__NOTE:__ You can use: `WORD_RE = re.compile(r\"[\\w']+\")` to tokenize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.A SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing onerecord.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile onerecord.txt\n",
    "foo foo quux labs foo bar quux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper3.2.A.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper3.2.A.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32AMAPPER\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# To count how many times mapper is called\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")  # to tokenize\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    for word in WORD_RE.findall(line.lower()):\n",
    "        print '%s\\t%s' % (word, 1)\n",
    "        \n",
    "\n",
    "# END STUDENT CODE HW32AMAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer3.2.A.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer3.2.A.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32AREDUCER\n",
    "\n",
    "import sys\n",
    "\n",
    "# To count how many times reducer is called\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    if current_word == word:\n",
    "        current_count += int(count)\n",
    "    else:\n",
    "        if current_word:\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = int(count)\n",
    "        current_word = word\n",
    "\n",
    "# Output the last word if needed\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)\n",
    "\n",
    "# END STUDENT CODE HW32AREDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper3.2.A.py\n",
    "!chmod a+x reducer3.2.A.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted onerecord.txt\n",
      "Deleted HW3.2A/results\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob4643666819972053817.jar tmpDir=null\n",
      "17/01/28 17:29:49 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/28 17:29:49 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/28 17:29:50 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/28 17:29:50 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "17/01/28 17:29:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485626579957_0007\n",
      "17/01/28 17:29:51 INFO impl.YarnClientImpl: Submitted application application_1485626579957_0007\n",
      "17/01/28 17:29:51 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1485626579957_0007/\n",
      "17/01/28 17:29:51 INFO mapreduce.Job: Running job: job_1485626579957_0007\n",
      "17/01/28 17:29:59 INFO mapreduce.Job: Job job_1485626579957_0007 running in uber mode : false\n",
      "17/01/28 17:29:59 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/28 17:30:04 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/28 17:30:14 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "17/01/28 17:30:17 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "17/01/28 17:30:19 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/28 17:30:20 INFO mapreduce.Job: Job job_1485626579957_0007 completed successfully\n",
      "17/01/28 17:30:20 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=83\n",
      "\t\tFILE: Number of bytes written=600003\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=137\n",
      "\t\tHDFS: Number of bytes written=26\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=2984\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=35869\n",
      "\t\tTotal time spent by all map tasks (ms)=2984\n",
      "\t\tTotal time spent by all reduce tasks (ms)=35869\n",
      "\t\tTotal vcore-seconds taken by all map tasks=2984\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=35869\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=3055616\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=36729856\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=45\n",
      "\t\tMap output materialized bytes=83\n",
      "\t\tInput split bytes=107\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=83\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=312\n",
      "\t\tCPU time spent (ms)=4640\n",
      "\t\tPhysical memory (bytes) snapshot=998162432\n",
      "\t\tVirtual memory (bytes) snapshot=7798841344\n",
      "\t\tTotal committed heap usage (bytes)=782237696\n",
      "\tMapper Counters\n",
      "\t\tCalls=1\n",
      "\tReducer Counters\n",
      "\t\tCalls=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=30\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26\n",
      "17/01/28 17:30:20 INFO streaming.StreamJob: Output directory: HW3.2A/results\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW32AHADOOP\n",
    "\n",
    "!hdfs dfs -rm onerecord.txt # Comment when running first time\n",
    "!hdfs dfs -copyFromLocal onerecord.txt\n",
    "!hdfs dfs -rm -r HW3.2A/results # Comment only when running first time\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "  -D mapreduce.job.name=\"Testing Count of Mappers and Reducers via Streaming\" \\\n",
    "  -D mapreduce.job.maps=1 \\\n",
    "  -files mapper3.2.A.py,reducer3.2.A.py \\\n",
    "  -mapper mapper3.2.A.py \\\n",
    "  -reducer reducer3.2.A.py \\\n",
    "  -input onerecord.txt \\\n",
    "  -output HW3.2A/results \\\n",
    "  -numReduceTasks 4\n",
    "\n",
    "# END STUDENT CODE HW32AHADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "# INSERT SCREENSHOT OF JOB TRACKER UI COUNTERS\n",
       "<H1> Mapper and Reducer Counters - Job Tracker UI Screenshot </H1>\n",
       "<img src=\"counters32a.png\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "# INSERT SCREENSHOT OF JOB TRACKER UI COUNTERS\n",
    "<H1> Mapper and Reducer Counters - Job Tracker UI Screenshot </H1>\n",
    "<img src=\"counters32a.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.2.A EXPLANATION\n",
    "When the number of mappers and reducers are not specified, the counters indicate 2 calls for the mapper and 1 call for the reducer by default. The number of mappers depends on the number of blocks in hdfs when the input file is stored.\n",
    "\n",
    "In order to obtain specifically 1 mapper and 4 reducers, we need to specify the number of mappers and reducers as option when running the hadoop streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.B SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper3.2.B.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper3.2.B.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32BMAPPER\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# To count how many times mapper is called\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")  # to tokenize\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    complaint_ID, product, sub_product, issue, other = line.split(\",\", 4)\n",
    "    for word in WORD_RE.findall(issue.lower()):\n",
    "        print '%s\\t%s' % (word, 1)\n",
    "\n",
    "# END STUDENT CODE HW32BMAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer3.2.B.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer3.2.B.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32BREDUCER\n",
    "\n",
    "import sys\n",
    "\n",
    "# To count how many times reducer is called\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    if current_word == word:\n",
    "        current_count += int(count)\n",
    "    else:\n",
    "        if current_word:\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = int(count)\n",
    "        current_word = word\n",
    "\n",
    "# Output the last word if needed\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)\n",
    "\n",
    "# END STUDENT CODE HW32BREDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper3.2.B.py\n",
    "!chmod a+x reducer3.2.B.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted ccd.csv\n",
      "Deleted HW3.2B/results\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob574503737770814300.jar tmpDir=null\n",
      "17/01/29 11:21:11 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 11:21:12 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 11:21:12 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/29 11:21:13 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "17/01/29 11:21:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485626579957_0009\n",
      "17/01/29 11:21:13 INFO impl.YarnClientImpl: Submitted application application_1485626579957_0009\n",
      "17/01/29 11:21:13 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1485626579957_0009/\n",
      "17/01/29 11:21:13 INFO mapreduce.Job: Running job: job_1485626579957_0009\n",
      "17/01/29 11:21:19 INFO mapreduce.Job: Job job_1485626579957_0009 running in uber mode : false\n",
      "17/01/29 11:21:19 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/29 11:21:30 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "17/01/29 11:21:35 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/01/29 11:21:38 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/29 11:21:44 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "17/01/29 11:21:45 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/29 11:21:46 INFO mapreduce.Job: Job job_1485626579957_0009 completed successfully\n",
      "17/01/29 11:21:46 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=11233487\n",
      "\t\tFILE: Number of bytes written=23186942\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50919178\n",
      "\t\tHDFS: Number of bytes written=2091\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=4\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=48019\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=23280\n",
      "\t\tTotal time spent by all map tasks (ms)=48019\n",
      "\t\tTotal time spent by all reduce tasks (ms)=23280\n",
      "\t\tTotal vcore-seconds taken by all map tasks=48019\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=23280\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=49171456\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=23838720\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=980483\n",
      "\t\tMap output bytes=9272509\n",
      "\t\tMap output materialized bytes=11233523\n",
      "\t\tInput split bytes=404\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=169\n",
      "\t\tReduce shuffle bytes=11233523\n",
      "\t\tReduce input records=980483\n",
      "\t\tReduce output records=169\n",
      "\t\tSpilled Records=1960966\n",
      "\t\tShuffled Maps =8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tGC time elapsed (ms)=569\n",
      "\t\tCPU time spent (ms)=11080\n",
      "\t\tPhysical memory (bytes) snapshot=1516015616\n",
      "\t\tVirtual memory (bytes) snapshot=9388638208\n",
      "\t\tTotal committed heap usage (bytes)=1341652992\n",
      "\tMapper Counters\n",
      "\t\tCalls=4\n",
      "\tReducer Counters\n",
      "\t\tCalls=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50918774\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2091\n",
      "17/01/29 11:21:46 INFO streaming.StreamJob: Output directory: HW3.2B/results\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW32BHADOOP\n",
    "\n",
    "!hdfs dfs -rm ccd.csv # Comment when running first time\n",
    "!hdfs dfs -copyFromLocal ccd.csv\n",
    "!hdfs dfs -rm -r HW3.2B/results # Comment only when running first time\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "  -D mapreduce.job.name=\"Testing Count of Mappers and Reducers via Streaming\" \\\n",
    "  -D mapreduce.job.maps=4 \\\n",
    "  -files mapper3.2.B.py,reducer3.2.B.py \\\n",
    "  -mapper mapper3.2.B.py \\\n",
    "  -reducer reducer3.2.B.py \\\n",
    "  -input ccd.csv \\\n",
    "  -output HW3.2B/results \\\n",
    "  -numReduceTasks 2\n",
    "\n",
    "# END STUDENT CODE HW32BHADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.2.B OUTPUT/ANSWER\n",
    "# The value of my user defined Mapper Counter is 4 and the value of Reducer Counter is 2.\n",
    "# These values are the same as the ones defined in the hadoop streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "# INSERT SCREENSHOT OF JOB TRACKER UI COUNTERS\n",
       "<H1> 3.2.B Mapper and Reducer Counters - Job Tracker UI Screenshot </H1>\n",
       "<img src=\"counters32b.png\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "# INSERT SCREENSHOT OF JOB TRACKER UI COUNTERS\n",
    "<H1> 3.2.B Mapper and Reducer Counters - Job Tracker UI Screenshot </H1>\n",
    "<img src=\"counters32b.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.C SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper3.2.C.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper3.2.C.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CMAPPER\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# To count how many times mapper is called\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")  # to tokenize\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    complaint_ID, product, sub_product, issue, other = line.split(\",\", 4)\n",
    "    for word in WORD_RE.findall(issue.lower()):\n",
    "        print '%s\\t%s' % (word, 1)\n",
    "\n",
    "# END STUDENT CODE HW32CMAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing combiner3.2.C.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner3.2.C.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CCOMBINER\n",
    "\n",
    "# Note: the combiner is the same as the reducer in this word count exercise\n",
    "# The only difference is the user defined counter for combiners\n",
    "\n",
    "import sys\n",
    "\n",
    "# To count how many times combiner is called\n",
    "sys.stderr.write(\"reporter:counter:Combiner Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    if current_word == word:\n",
    "        current_count += int(count)\n",
    "    else:\n",
    "        if current_word:\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = int(count)\n",
    "        current_word = word\n",
    "\n",
    "# Output the last word if needed\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)\n",
    "\n",
    "# END STUDENT CODE HW32CCOMBINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer3.2.C.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer3.2.C.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CREDUCER\n",
    "\n",
    "import sys\n",
    "\n",
    "# To count how many times reducer is called\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    if current_word == word:\n",
    "        current_count += int(count)\n",
    "    else:\n",
    "        if current_word:\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = int(count)\n",
    "        current_word = word\n",
    "\n",
    "# Output the last word if needed\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)\n",
    "\n",
    "# END STUDENT CODE HW32CREDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper3.2.C.py\n",
    "!chmod a+x combiner3.2.C.py\n",
    "!chmod a+x reducer3.2.C.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted ccd.csv\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob6977870791111990717.jar tmpDir=null\n",
      "17/01/29 12:01:05 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 12:01:05 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 12:01:06 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/29 12:01:06 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/29 12:01:06 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485626579957_0010\n",
      "17/01/29 12:01:06 INFO impl.YarnClientImpl: Submitted application application_1485626579957_0010\n",
      "17/01/29 12:01:07 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1485626579957_0010/\n",
      "17/01/29 12:01:07 INFO mapreduce.Job: Running job: job_1485626579957_0010\n",
      "17/01/29 12:01:13 INFO mapreduce.Job: Job job_1485626579957_0010 running in uber mode : false\n",
      "17/01/29 12:01:13 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/29 12:01:23 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/01/29 12:01:24 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/29 12:01:30 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/29 12:01:30 INFO mapreduce.Job: Job job_1485626579957_0010 completed successfully\n",
      "17/01/29 12:01:30 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4438\n",
      "\t\tFILE: Number of bytes written=370726\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910784\n",
      "\t\tHDFS: Number of bytes written=2091\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16583\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4334\n",
      "\t\tTotal time spent by all map tasks (ms)=16583\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4334\n",
      "\t\tTotal vcore-seconds taken by all map tasks=16583\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4334\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=16980992\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4438016\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=980483\n",
      "\t\tMap output bytes=9272509\n",
      "\t\tMap output materialized bytes=4444\n",
      "\t\tInput split bytes=202\n",
      "\t\tCombine input records=980483\n",
      "\t\tCombine output records=313\n",
      "\t\tReduce input groups=169\n",
      "\t\tReduce shuffle bytes=4444\n",
      "\t\tReduce input records=313\n",
      "\t\tReduce output records=169\n",
      "\t\tSpilled Records=626\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=168\n",
      "\t\tCPU time spent (ms)=5380\n",
      "\t\tPhysical memory (bytes) snapshot=724000768\n",
      "\t\tVirtual memory (bytes) snapshot=4663554048\n",
      "\t\tTotal committed heap usage (bytes)=672137216\n",
      "\tCombiner Counters\n",
      "\t\tCalls=2\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2091\n",
      "17/01/29 12:01:30 INFO streaming.StreamJob: Output directory: HW3.2C/results\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW32CHADOOP\n",
    "\n",
    "!hdfs dfs -rm ccd.csv # Comment when running first time\n",
    "!hdfs dfs -copyFromLocal ccd.csv\n",
    "#!hdfs dfs -rm -r HW3.2C/results # Comment only when running first time\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "  -D mapreduce.job.name=\"Word Count with Combiners\" \\\n",
    "  -files mapper3.2.C.py,reducer3.2.C.py,combiner3.2.C.py \\\n",
    "  -mapper mapper3.2.C.py \\\n",
    "  -reducer reducer3.2.C.py \\\n",
    "  -combiner combiner3.2.C.py \\\n",
    "  -input ccd.csv \\\n",
    "  -output HW3.2C/results \\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "# END STUDENT CODE HW32CHADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.2.C OUTPUT/ANSWER\n",
    "#The value of the unser defined counters for:\n",
    "#    - mapper is 2\n",
    "#    - combiner is 2\n",
    "#    - reducer is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "# INSERT SCREENSHOT OF JOB TRACKER UI COUNTERS\n",
       "<H1> 3.2.C Mappers and Reducers Counter - Job tracker UI </H1>\n",
       "<img src=\"counters32c.png\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "# INSERT SCREENSHOT OF JOB TRACKER UI COUNTERS\n",
    "<H1> 3.2.C Mappers and Reducers Counter - Job tracker UI </H1>\n",
    "<img src=\"counters32c.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frequencies_mapper3.2.C.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frequencies_mapper3.2.C.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CFREQMAPPER\n",
    "\n",
    "import sys\n",
    "import re\n",
    "    \n",
    "# To count how many times mapper is called\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "total_count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")  # to tokenize\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    complaint_ID, product, sub_product, issue, other = line.split(\",\", 4)\n",
    "    for word in WORD_RE.findall(issue.lower()):\n",
    "        print '%s\\t%s' % (word, 1)\n",
    "        total_count += 1\n",
    "\n",
    "print '%s\\t%s' % (\"** totalcount\", total_count)\n",
    "\n",
    "# END STUDENT CODE HW32CFREQMAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frequencies_reducer3.2.C.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frequencies_reducer3.2.C.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CFREQREDUCER\n",
    "\n",
    "import sys\n",
    "\n",
    "# To count how many times reducer is called\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    if current_word == word:\n",
    "        current_count += int(count)\n",
    "    else:\n",
    "        if current_word:\n",
    "            if current_word == \"** totalcount\":\n",
    "                total_count = int(current_count)\n",
    "            else:               \n",
    "                print '%s\\t%s\\t%s' % (current_word, current_count, float(current_count)/float(total_count))\n",
    "        current_count = int(count)\n",
    "        current_word = word\n",
    "\n",
    "# Output the last word if needed\n",
    "if current_word == word:\n",
    "    print '%s\\t%s\\t%s' % (current_word, current_count, float(current_count)/float(total_count))\n",
    "\n",
    "# END STUDENT CODE HW32CFREQREDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod a+x frequencies_mapper3.2.C.py\n",
    "!chmod a+x frequencies_reducer3.2.C.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted ccd.csv\n",
      "Deleted HW3.2C2/results\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob2104237707239670910.jar tmpDir=null\n",
      "17/01/29 14:04:30 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 14:04:30 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 14:04:32 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/29 14:04:32 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/29 14:04:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485626579957_0015\n",
      "17/01/29 14:04:32 INFO impl.YarnClientImpl: Submitted application application_1485626579957_0015\n",
      "17/01/29 14:04:32 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1485626579957_0015/\n",
      "17/01/29 14:04:32 INFO mapreduce.Job: Running job: job_1485626579957_0015\n",
      "17/01/29 14:04:39 INFO mapreduce.Job: Job job_1485626579957_0015 running in uber mode : false\n",
      "17/01/29 14:04:39 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/29 14:04:49 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/01/29 14:04:50 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/29 14:04:58 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/29 14:04:59 INFO mapreduce.Job: Job job_1485626579957_0015 completed successfully\n",
      "17/01/29 14:04:59 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=11233527\n",
      "\t\tFILE: Number of bytes written=22827356\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910784\n",
      "\t\tHDFS: Number of bytes written=4979\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16471\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6241\n",
      "\t\tTotal time spent by all map tasks (ms)=16471\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6241\n",
      "\t\tTotal vcore-seconds taken by all map tasks=16471\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=6241\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=16866304\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6390784\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=980485\n",
      "\t\tMap output bytes=9272551\n",
      "\t\tMap output materialized bytes=11233533\n",
      "\t\tInput split bytes=202\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=170\n",
      "\t\tReduce shuffle bytes=11233533\n",
      "\t\tReduce input records=980485\n",
      "\t\tReduce output records=169\n",
      "\t\tSpilled Records=1960970\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=211\n",
      "\t\tCPU time spent (ms)=7290\n",
      "\t\tPhysical memory (bytes) snapshot=776962048\n",
      "\t\tVirtual memory (bytes) snapshot=4682383360\n",
      "\t\tTotal committed heap usage (bytes)=735051776\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4979\n",
      "17/01/29 14:04:59 INFO streaming.StreamJob: Output directory: HW3.2C2/results\n",
      "Deleted 32C_non_sorted.txt\n",
      "Deleted HW3.2C2sorted/results\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob2034274374760484414.jar tmpDir=null\n",
      "17/01/29 14:05:12 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 14:05:12 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 14:05:13 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/29 14:05:13 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/29 14:05:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485626579957_0016\n",
      "17/01/29 14:05:13 INFO impl.YarnClientImpl: Submitted application application_1485626579957_0016\n",
      "17/01/29 14:05:13 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1485626579957_0016/\n",
      "17/01/29 14:05:13 INFO mapreduce.Job: Running job: job_1485626579957_0016\n",
      "17/01/29 14:05:20 INFO mapreduce.Job: Job job_1485626579957_0016 running in uber mode : false\n",
      "17/01/29 14:05:20 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/29 14:05:26 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/01/29 14:05:27 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/29 14:05:32 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/29 14:05:33 INFO mapreduce.Job: Job job_1485626579957_0016 completed successfully\n",
      "17/01/29 14:05:33 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5323\n",
      "\t\tFILE: Number of bytes written=368155\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7693\n",
      "\t\tHDFS: Number of bytes written=4979\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8387\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3308\n",
      "\t\tTotal time spent by all map tasks (ms)=8387\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3308\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8387\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3308\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8588288\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3387392\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=169\n",
      "\t\tMap output records=169\n",
      "\t\tMap output bytes=4979\n",
      "\t\tMap output materialized bytes=5329\n",
      "\t\tInput split bytes=224\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=169\n",
      "\t\tReduce shuffle bytes=5329\n",
      "\t\tReduce input records=169\n",
      "\t\tReduce output records=169\n",
      "\t\tSpilled Records=338\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=93\n",
      "\t\tCPU time spent (ms)=1870\n",
      "\t\tPhysical memory (bytes) snapshot=763568128\n",
      "\t\tVirtual memory (bytes) snapshot=4691075072\n",
      "\t\tTotal committed heap usage (bytes)=599261184\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=7469\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4979\n",
      "17/01/29 14:05:33 INFO streaming.StreamJob: Output directory: HW3.2C2sorted/results\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW32CFREQHADOOP\n",
    "\n",
    "### First MR job to sort the words alphabetically\n",
    "# Note: only one reducer so one partition thus no need for extra coding for total sort\n",
    "\n",
    "!hdfs dfs -rm ccd.csv # Comment when running first time\n",
    "!hdfs dfs -copyFromLocal ccd.csv\n",
    "!hdfs dfs -rm -r HW3.2C2/results # Comment only when running first time\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "  -D mapreduce.job.name=\"Word frequencies and relative frequencies sorted by word\" \\\n",
    "  -files frequencies_mapper3.2.C.py,frequencies_reducer3.2.C.py \\\n",
    "  -mapper frequencies_mapper3.2.C.py \\\n",
    "  -reducer frequencies_reducer3.2.C.py \\\n",
    "  -input ccd.csv \\\n",
    "  -output HW3.2C2/results \\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "!hdfs dfs -cat HW3.2C2/results/part-0000* > 32C_non_sorted.txt\n",
    "\n",
    "\n",
    "### Second MR job to sort the words by frequencies (and by words alphabetically if ties)\n",
    "\n",
    "!hdfs dfs -rm 32C_non_sorted.txt # Comment when running first time\n",
    "!hdfs dfs -copyFromLocal 32C_non_sorted.txt\n",
    "!hdfs dfs -rm -r HW3.2C2sorted/results # Comment only when running first time\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "  -D mapreduce.job.name=\"Word frequencies and relative frequencies sorted by frequencies\" \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D stream.map.output.key.field.separator=\"\\t\" \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input 32C_non_sorted.txt \\\n",
    "  -output HW3.2C2sorted/results \\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "!hdfs dfs -cat HW3.2C2sorted/results/part-0000* > 32C_sorted.txt\n",
    "\n",
    "# END STUDENT CODE HW32CFREQHADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 50 most frequent terms with their frequency and relative frequency:\n",
      "loan\t119630\t0.122011294433\n",
      "modification\t70487\t0.0718900786653\n",
      "credit\t55251\t0.056350798535\n",
      "servicing\t36767\t0.0374988653551\n",
      "report\t34903\t0.0355977615114\n",
      "incorrect\t29133\t0.0297129068021\n",
      "information\t29069\t0.0296476328503\n",
      "on\t29069\t0.0296476328503\n",
      "or\t22533\t0.0229815305314\n",
      "account\t20681\t0.0210926655536\n",
      "debt\t19309\t0.0196933552137\n",
      "and\t16448\t0.0167754055909\n",
      "opening\t16205\t0.0165275685555\n",
      "club\t12545\t0.0127947144418\n",
      "health\t12545\t0.0127947144418\n",
      "not\t12353\t0.0125988925866\n",
      "attempts\t11848\t0.0120838403114\n",
      "collect\t11848\t0.0120838403114\n",
      "cont'd\t11848\t0.0120838403114\n",
      "owed\t11848\t0.0120838403114\n",
      "of\t10885\t0.0111016713191\n",
      "my\t10731\t0.0109446058728\n",
      "deposits\t10555\t0.0107651025056\n",
      "withdrawals\t10555\t0.0107651025056\n",
      "problems\t9484\t0.00967278371986\n",
      "application\t8868\t0.0090445219346\n",
      "to\t8401\t0.00856822606817\n",
      "unable\t8178\t0.00834078714266\n",
      "billing\t8158\t0.00832038903275\n",
      "other\t7886\t0.00804297473796\n",
      "disputes\t6938\t0.00707610432817\n",
      "communication\t6920\t0.00705774602925\n",
      "tactics\t6920\t0.00705774602925\n",
      "reporting\t6559\t0.00668956014536\n",
      "lease\t6337\t0.00646314112534\n",
      "the\t6248\t0.00637236953624\n",
      "being\t5663\t0.00577572482134\n",
      "by\t5663\t0.00577572482134\n",
      "caused\t5663\t0.00577572482134\n",
      "funds\t5663\t0.00577572482134\n",
      "low\t5663\t0.00577572482134\n",
      "process\t5505\t0.00561457975304\n",
      "disclosure\t5214\t0.00531778725383\n",
      "verification\t5214\t0.00531778725383\n",
      "managing\t5006\t0.00510564691076\n",
      "company's\t4858\t0.00495470089741\n",
      "investigation\t4858\t0.00495470089741\n",
      "identity\t4729\t0.00482313308849\n",
      "card\t4405\t0.00449268370793\n",
      "get\t4357\t0.00444372824414\n",
      "\n",
      "The bottom 10 tokens with their frequency and relative frequency\n",
      "apply\t118\t0.000120348848476\n",
      "amount\t98\t9.99507385646e-05\n",
      "credited\t92\t9.38313055912e-05\n",
      "payment\t92\t9.38313055912e-05\n",
      "checks\t75\t7.64929121668e-05\n",
      "convenience\t75\t7.64929121668e-05\n",
      "amt\t71\t7.24132901845e-05\n",
      "day\t71\t7.24132901845e-05\n",
      "disclosures\t64\t6.52739517156e-05\n",
      "missing\t64\t6.52739517156e-05\n"
     ]
    }
   ],
   "source": [
    "# 3.2.C OUTPUT/ANSWER\n",
    "print \"The top 50 most frequent terms with their frequency and relative frequency:\"\n",
    "!head -50 32C_sorted.txt\n",
    "\n",
    "print \"\\nThe bottom 10 tokens with their frequency and relative frequency\"\n",
    "!tail -10 32C_sorted.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.2.1\"></a>\n",
    "### 3.2.1  \n",
    "Using **2 reducers**: What are the top **50 most frequent terms** in your word count analysis? \n",
    "\n",
    "Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). Please **use a combiner.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### START STUDENT CODE HW321 (INSERT CELLS BELOW AS NEEDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frequencies_mapper3.2.1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frequencies_mapper3.2.1.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "    \n",
    "# To count how many times mapper is called\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "total_count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")  # to tokenize\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    complaint_ID, product, sub_product, issue, other = line.split(\",\", 4)\n",
    "    for word in WORD_RE.findall(issue.lower()):\n",
    "        # 2 reducers means 2 partitions, so need to group words into 2\n",
    "        # Assuming all words are uniformly distributed from a to z, middle letters are m and n to divide into\n",
    "        # roughly 2 equal size partitions\n",
    "        if word[0] >= \"n\":\n",
    "            group = \"B\"\n",
    "        else:\n",
    "            group = \"A\"\n",
    "        #group = \"group\" + str(int(word[0] >= \"n\")) # if first letter before letter \"n\" then group0, else group 1\n",
    "        print '%s\\t%s\\t%s' % (group, word, 1)\n",
    "        \n",
    "        total_count += 1\n",
    "\n",
    "# Repeat the total count in each partition so that the relative frequencies of words within each reducer\n",
    "# can be calculated with the total of words across all partitions\n",
    "print '%s\\t%s\\t%s' % (\"A\", \"** totalcount\", total_count)\n",
    "print '%s\\t%s\\t%s' % (\"B\", \"** totalcount\", total_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frequencies_combiner3.2.1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frequencies_combiner3.2.1.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "# To count how many times combiner is called\n",
    "sys.stderr.write(\"reporter:counter:Combiner Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    group, word, count = line.split(\"\\t\")\n",
    "   \n",
    "    if current_word == word:\n",
    "        current_count += int(count)\n",
    "    else:\n",
    "        if current_word:             \n",
    "            print '%s\\t%s\\t%s' % (group, current_word, current_count)\n",
    "        current_count = int(count)\n",
    "        current_word = word\n",
    "\n",
    "# Output the last word if needed\n",
    "if current_word == word:\n",
    "    print '%s\\t%s\\t%s' % (group, current_word, current_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frequencies_reducer3.2.1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frequencies_reducer3.2.1.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "# To count how many times reducer is called\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    group, word, count = line.split(\"\\t\")   # Note: will toss out \"group\"\n",
    "    \n",
    "    if word == \"** totalcount\":\n",
    "        total_count += int(count)\n",
    "   \n",
    "    elif current_word == word:\n",
    "        current_count += int(count)\n",
    "    else:\n",
    "        if current_word:\n",
    "            relative_freq = float(current_count)/float(total_count)\n",
    "            print '%s\\t%s\\t%s' % (current_word, current_count, \"{0:.5f}\".format(relative_freq))\n",
    "        current_count = int(count)\n",
    "        current_word = word\n",
    "\n",
    "\n",
    "relative_freq = float(current_count)/float(total_count)\n",
    "print '%s\\t%s\\t%s' % (current_word, current_count, \"{0:.5f}\".format(relative_freq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x frequencies_mapper3.2.1.py\n",
    "!chmod a+x frequencies_combiner3.2.1.py\n",
    "!chmod a+x frequencies_reducer3.2.1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted ccd.csv\n",
      "Deleted HW3.2.1/results\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob3170883263828537388.jar tmpDir=null\n",
      "17/01/30 18:13:28 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/30 18:13:28 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/30 18:13:29 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/30 18:13:29 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/30 18:13:30 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485626579957_0075\n",
      "17/01/30 18:13:30 INFO impl.YarnClientImpl: Submitted application application_1485626579957_0075\n",
      "17/01/30 18:13:30 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1485626579957_0075/\n",
      "17/01/30 18:13:30 INFO mapreduce.Job: Running job: job_1485626579957_0075\n",
      "17/01/30 18:13:37 INFO mapreduce.Job: Job job_1485626579957_0075 running in uber mode : false\n",
      "17/01/30 18:13:37 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/30 18:13:54 INFO mapreduce.Job:  map 16% reduce 0%\n",
      "17/01/30 18:13:57 INFO mapreduce.Job:  map 36% reduce 0%\n",
      "17/01/30 18:13:59 INFO mapreduce.Job:  map 54% reduce 0%\n",
      "17/01/30 18:14:00 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "17/01/30 18:14:04 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "17/01/30 18:14:05 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/30 18:14:14 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/30 18:14:16 INFO mapreduce.Job: Job job_1485626579957_0075 completed successfully\n",
      "17/01/30 18:14:16 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5170\n",
      "\t\tFILE: Number of bytes written=497514\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910784\n",
      "\t\tHDFS: Number of bytes written=3443\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=49607\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=14811\n",
      "\t\tTotal time spent by all map tasks (ms)=49607\n",
      "\t\tTotal time spent by all reduce tasks (ms)=14811\n",
      "\t\tTotal vcore-seconds taken by all map tasks=49607\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=14811\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=50797568\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=15166464\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=980487\n",
      "\t\tMap output bytes=11233567\n",
      "\t\tMap output materialized bytes=5182\n",
      "\t\tInput split bytes=202\n",
      "\t\tCombine input records=980487\n",
      "\t\tCombine output records=317\n",
      "\t\tReduce input groups=171\n",
      "\t\tReduce shuffle bytes=5182\n",
      "\t\tReduce input records=317\n",
      "\t\tReduce output records=169\n",
      "\t\tSpilled Records=634\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=2542\n",
      "\t\tCPU time spent (ms)=12530\n",
      "\t\tPhysical memory (bytes) snapshot=1142489088\n",
      "\t\tVirtual memory (bytes) snapshot=6257762304\n",
      "\t\tTotal committed heap usage (bytes)=1043857408\n",
      "\tCombiner Counters\n",
      "\t\tCalls=4\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3443\n",
      "17/01/30 18:14:16 INFO streaming.StreamJob: Output directory: HW3.2.1/results\n",
      "Deleted 321_non_sorted_freq.txt\n",
      "Deleted HW3.2.1sorted/results\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob988652563739770779.jar tmpDir=null\n",
      "17/01/30 18:14:30 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/30 18:14:30 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/30 18:14:31 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/30 18:14:31 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/30 18:14:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485626579957_0076\n",
      "17/01/30 18:14:32 INFO impl.YarnClientImpl: Submitted application application_1485626579957_0076\n",
      "17/01/30 18:14:32 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1485626579957_0076/\n",
      "17/01/30 18:14:32 INFO mapreduce.Job: Running job: job_1485626579957_0076\n",
      "17/01/30 18:14:39 INFO mapreduce.Job: Job job_1485626579957_0076 running in uber mode : false\n",
      "17/01/30 18:14:39 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/30 18:14:46 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/01/30 18:14:47 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/30 18:14:52 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/30 18:14:52 INFO mapreduce.Job: Job job_1485626579957_0076 completed successfully\n",
      "17/01/30 18:14:52 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3787\n",
      "\t\tFILE: Number of bytes written=365098\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5399\n",
      "\t\tHDFS: Number of bytes written=3443\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10146\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3494\n",
      "\t\tTotal time spent by all map tasks (ms)=10146\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3494\n",
      "\t\tTotal vcore-seconds taken by all map tasks=10146\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3494\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=10389504\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3577856\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=169\n",
      "\t\tMap output records=169\n",
      "\t\tMap output bytes=3443\n",
      "\t\tMap output materialized bytes=3793\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=169\n",
      "\t\tReduce shuffle bytes=3793\n",
      "\t\tReduce input records=169\n",
      "\t\tReduce output records=169\n",
      "\t\tSpilled Records=338\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=123\n",
      "\t\tCPU time spent (ms)=1980\n",
      "\t\tPhysical memory (bytes) snapshot=762556416\n",
      "\t\tVirtual memory (bytes) snapshot=4701859840\n",
      "\t\tTotal committed heap usage (bytes)=602406912\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5165\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3443\n",
      "17/01/30 18:14:52 INFO streaming.StreamJob: Output directory: HW3.2.1sorted/results\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "\n",
    "### First MR job to sort the words alphabetically\n",
    "# Note: 2 reducers so need extra coding for total sort with prepending to identify 2 groups\n",
    "# and specify 2 keys to sort (group and word)\n",
    "\n",
    "!hdfs dfs -rm ccd.csv # Comment when running first time\n",
    "!hdfs dfs -copyFromLocal ccd.csv\n",
    "!hdfs dfs -rm -r HW3.2.1/results # Comment only when running first time\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D stream.num.reduce.output.key.fields=2 \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2\" \\\n",
    "  -files frequencies_mapper3.2.1.py,frequencies_reducer3.2.1.py,frequencies_combiner3.2.1.py \\\n",
    "  -mapper frequencies_mapper3.2.1.py \\\n",
    "  -reducer frequencies_reducer3.2.1.py \\\n",
    "  -combiner frequencies_combiner3.2.1.py \\\n",
    "  -input ccd.csv \\\n",
    "  -output HW3.2.1/results \\\n",
    "  -numReduceTasks 2 \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner\n",
    "\n",
    "\n",
    "!hdfs dfs -cat HW3.2.1/results/part-0000* > 321_non_sorted_freq.txt\n",
    "\n",
    "\n",
    "### Second MR job to sort the words by frequencies (and by words alphabetically if ties)\n",
    "\n",
    "!hdfs dfs -rm 321_non_sorted_freq.txt # Comment when running first time\n",
    "!hdfs dfs -copyFromLocal 321_non_sorted_freq.txt\n",
    "!hdfs dfs -rm -r HW3.2.1sorted/results # Comment only when running first time\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "  -D mapreduce.job.name=\"Word frequencies and relative frequencies sorted by frequencies\" \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D stream.map.output.key.field.separator=\"\\t\" \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input 321_non_sorted_freq.txt \\\n",
    "  -output HW3.2.1sorted/results \\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "!hdfs dfs -cat HW3.2.1sorted/results/part-0000* > 321_sorted_freq.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 Reducers\n",
      "The top 50 most frequent terms with their frequency and relative frequency:\n",
      "loan\t119630\t0.12201\r\n",
      "modification\t70487\t0.07189\r\n",
      "credit\t55251\t0.05635\r\n",
      "servicing\t36767\t0.03750\r\n",
      "report\t34903\t0.03560\r\n",
      "incorrect\t29133\t0.02971\r\n",
      "information\t29069\t0.02965\r\n",
      "on\t29069\t0.02965\r\n",
      "or\t22533\t0.02298\r\n",
      "account\t20681\t0.02109\r\n",
      "debt\t19309\t0.01969\r\n",
      "and\t16448\t0.01678\r\n",
      "opening\t16205\t0.01653\r\n",
      "club\t12545\t0.01279\r\n",
      "health\t12545\t0.01279\r\n",
      "not\t12353\t0.01260\r\n",
      "attempts\t11848\t0.01208\r\n",
      "collect\t11848\t0.01208\r\n",
      "cont'd\t11848\t0.01208\r\n",
      "owed\t11848\t0.01208\r\n",
      "of\t10885\t0.01110\r\n",
      "my\t10731\t0.01094\r\n",
      "deposits\t10555\t0.01077\r\n",
      "withdrawals\t10555\t0.01077\r\n",
      "problems\t9484\t0.00967\r\n",
      "application\t8868\t0.00904\r\n",
      "to\t8401\t0.00857\r\n",
      "unable\t8178\t0.00834\r\n",
      "billing\t8158\t0.00832\r\n",
      "other\t7886\t0.00804\r\n",
      "disputes\t6938\t0.00708\r\n",
      "communication\t6920\t0.00706\r\n",
      "tactics\t6920\t0.00706\r\n",
      "reporting\t6559\t0.00669\r\n",
      "lease\t6337\t0.00646\r\n",
      "the\t6248\t0.00637\r\n",
      "being\t5663\t0.00578\r\n",
      "by\t5663\t0.00578\r\n",
      "caused\t5663\t0.00578\r\n",
      "funds\t5663\t0.00578\r\n",
      "low\t5663\t0.00578\r\n",
      "process\t5505\t0.00561\r\n",
      "disclosure\t5214\t0.00532\r\n",
      "verification\t5214\t0.00532\r\n",
      "managing\t5006\t0.00511\r\n",
      "company's\t4858\t0.00495\r\n",
      "investigation\t4858\t0.00495\r\n",
      "identity\t4729\t0.00482\r\n",
      "card\t4405\t0.00449\r\n",
      "get\t4357\t0.00444\r\n"
     ]
    }
   ],
   "source": [
    "print \"Using 2 Reducers\"\n",
    "print \"The top 50 most frequent terms with their frequency and relative frequency:\"\n",
    "!head -50 321_sorted_freq.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END STUDENT CODE HW321"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.3\"></a>\n",
    "## HW3.3. Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\t\n",
    "For this homework use the online browsing behavior dataset located at: \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData \n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \n",
    "\n",
    "\n",
    "Do some exploratory data analysis of this dataset guided by the following questions:. \n",
    "\n",
    "How many unique items are available from this supplier?\n",
    "\n",
    "Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### START STUDENT CODE HW33 (INSERT CELLS BELOW AS NEEDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 3377k  100 3377k    0     0  1540k      0  0:00:02  0:00:02 --:--:-- 3838k\n"
     ]
    }
   ],
   "source": [
    "# First download data from dropbox and create csv file\n",
    "!curl 'https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0' -L -o cart.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing unique_mapper3.3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile unique_mapper3.3.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "    \n",
    "# To count how many times mapper is called\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    products = line.split()\n",
    "    for product in products:\n",
    "        print '%s\\t%s' % (product, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing unique_reducer3.3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile unique_reducer3.3.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "# To count how many times reducer is called\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "current_product = None\n",
    "total_unique_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    product, count = line.split('\\t', 1)\n",
    "\n",
    "    if current_product == product:\n",
    "        continue\n",
    "    else:\n",
    "        if current_product:\n",
    "            total_unique_count += 1  \n",
    "        current_product = product\n",
    "\n",
    "# Output the last product if needed\n",
    "if current_product == product:\n",
    "    total_unique_count += 1 \n",
    "    \n",
    "print '%s\\t%s' % (\"Number of unique products\", total_unique_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x unique_mapper3.3.py\n",
    "!chmod a+x unique_reducer3.3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted cart.txt\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob5364230983275209578.jar tmpDir=null\n",
      "17/01/29 20:20:46 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 20:20:46 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 20:20:48 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/29 20:20:48 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/29 20:20:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485626579957_0039\n",
      "17/01/29 20:20:49 INFO impl.YarnClientImpl: Submitted application application_1485626579957_0039\n",
      "17/01/29 20:20:49 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1485626579957_0039/\n",
      "17/01/29 20:20:49 INFO mapreduce.Job: Running job: job_1485626579957_0039\n",
      "17/01/29 20:20:56 INFO mapreduce.Job: Job job_1485626579957_0039 running in uber mode : false\n",
      "17/01/29 20:20:56 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/29 20:21:05 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/01/29 20:21:08 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/29 20:21:13 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/29 20:21:14 INFO mapreduce.Job: Job job_1485626579957_0039 completed successfully\n",
      "17/01/29 20:21:14 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4950718\n",
      "\t\tFILE: Number of bytes written=10261447\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462817\n",
      "\t\tHDFS: Number of bytes written=32\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=15032\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5496\n",
      "\t\tTotal time spent by all map tasks (ms)=15032\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5496\n",
      "\t\tTotal vcore-seconds taken by all map tasks=15032\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5496\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=15392768\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5627904\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380824\n",
      "\t\tMap output bytes=4189064\n",
      "\t\tMap output materialized bytes=4950724\n",
      "\t\tInput split bytes=204\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12592\n",
      "\t\tReduce shuffle bytes=4950724\n",
      "\t\tReduce input records=380824\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=761648\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=242\n",
      "\t\tCPU time spent (ms)=5760\n",
      "\t\tPhysical memory (bytes) snapshot=754102272\n",
      "\t\tVirtual memory (bytes) snapshot=4694294528\n",
      "\t\tTotal committed heap usage (bytes)=671612928\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=32\n",
      "17/01/29 20:21:14 INFO streaming.StreamJob: Output directory: HW3.3unique/results\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "\n",
    "### MR job to count number of unique products\n",
    "# Note: only one reducer so one partition thus no need for extra coding for total sort\n",
    "\n",
    "!hdfs dfs -rm cart.txt # Comment when running first time\n",
    "!hdfs dfs -copyFromLocal cart.txt\n",
    "#!hdfs dfs -rm -r HW3.3unique/results # Comment only when running first time\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "  -D mapreduce.job.name=\"Count of unique products\" \\\n",
    "  -files unique_mapper3.3.py,unique_reducer3.3.py \\\n",
    "  -mapper unique_mapper3.3.py \\\n",
    "  -reducer unique_reducer3.3.py \\\n",
    "  -input cart.txt \\\n",
    "  -output HW3.3unique/results \\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "!hdfs dfs -cat HW3.3unique/results/part-0000* > 33_unique.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique products\t12592\r\n"
     ]
    }
   ],
   "source": [
    "!head 33unique.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing basket_mapper3.3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile basket_mapper3.3.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "    \n",
    "# To count how many times mapper is called\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "basket_number = 1\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    count = len(line.split())\n",
    "    print '%s\\t%s' % (basket_number, count)\n",
    "    basket_number += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x basket_mapper3.3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted cart.txt\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob8268391173331036349.jar tmpDir=null\n",
      "17/01/29 20:02:10 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 20:02:10 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 20:02:11 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/29 20:02:11 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/29 20:02:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485626579957_0038\n",
      "17/01/29 20:02:11 INFO impl.YarnClientImpl: Submitted application application_1485626579957_0038\n",
      "17/01/29 20:02:11 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1485626579957_0038/\n",
      "17/01/29 20:02:11 INFO mapreduce.Job: Running job: job_1485626579957_0038\n",
      "17/01/29 20:02:18 INFO mapreduce.Job: Job job_1485626579957_0038 running in uber mode : false\n",
      "17/01/29 20:02:18 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/29 20:02:25 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/01/29 20:02:26 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/29 20:02:32 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/29 20:02:32 INFO mapreduce.Job: Job job_1485626579957_0038 completed successfully\n",
      "17/01/29 20:02:32 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=340409\n",
      "\t\tFILE: Number of bytes written=1041720\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462817\n",
      "\t\tHDFS: Number of bytes written=278201\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11727\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4555\n",
      "\t\tTotal time spent by all map tasks (ms)=11727\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4555\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11727\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4555\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=12008448\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4664320\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=31101\n",
      "\t\tMap output bytes=278201\n",
      "\t\tMap output materialized bytes=340415\n",
      "\t\tInput split bytes=204\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=30219\n",
      "\t\tReduce shuffle bytes=340415\n",
      "\t\tReduce input records=31101\n",
      "\t\tReduce output records=31101\n",
      "\t\tSpilled Records=62202\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=214\n",
      "\t\tCPU time spent (ms)=4750\n",
      "\t\tPhysical memory (bytes) snapshot=730267648\n",
      "\t\tVirtual memory (bytes) snapshot=4684562432\n",
      "\t\tTotal committed heap usage (bytes)=735051776\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=278201\n",
      "17/01/29 20:02:32 INFO streaming.StreamJob: Output directory: HW3.3basket/results\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "\n",
    "### MR job to count number of products per basket and sort by size\n",
    "# Note: only one reducer so one partition thus no need for extra coding for total sort\n",
    "\n",
    "!hdfs dfs -rm cart.txt # Comment when running first time\n",
    "!hdfs dfs -copyFromLocal cart.txt\n",
    "#!hdfs dfs -rm -r HW3.3basket/results # Comment only when running first time\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "  -D mapreduce.job.name=\"Basket size sorted by product count\" \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D stream.map.output.key.field.separator=\"\\t\" \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "  -files basket_mapper3.3.py \\\n",
    "  -mapper basket_mapper3.3.py \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input cart.txt \\\n",
    "  -output HW3.3basket/results \\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "!hdfs dfs -cat HW3.3basket/results/part-0000* > 33_basket.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest basket is (basket number, number of products in the basket)\n",
      "6914\t37\t\r\n"
     ]
    }
   ],
   "source": [
    "print \"The largest basket is (basket number, number of products in the basket)\"\n",
    "!head -1 33basket.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frequencies_mapper3.3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frequencies_mapper3.3.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "    \n",
    "# To count how many times mapper is called\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "total_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    products = line.split()\n",
    "    for product in products:\n",
    "        print '%s\\t%s' % (product, 1)\n",
    "        total_count += 1\n",
    "\n",
    "print '%s\\t%s' % (\"** totalcount\", total_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing frequencies_reducer3.3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frequencies_reducer3.3.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "# To count how many times reducer is called\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "current_product = None\n",
    "current_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    product, count = line.split('\\t', 1)\n",
    "\n",
    "    if current_product == product:\n",
    "        current_count += int(count)\n",
    "    else:\n",
    "        if current_product:\n",
    "            if current_product == \"** totalcount\":  # ** totalcount will at the top of input list since sorted\n",
    "                total_count = int(current_count)    # Thus, can be determined before passing over product list\n",
    "            else:               \n",
    "                print '%s\\t%s\\t%s' % (current_product, current_count, float(current_count)/float(total_count))\n",
    "        current_count = int(count)\n",
    "        current_product = product\n",
    "\n",
    "# Output the last product if needed\n",
    "if current_product == product:\n",
    "    print '%s\\t%s\\t%s' % (current_product, current_count, float(current_count)/float(total_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x frequencies_mapper3.3.py\n",
    "!chmod a+x frequencies_reducer3.3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted cart.txt\n",
      "Deleted HW3.3/results\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob7191364961557746772.jar tmpDir=null\n",
      "17/01/29 19:16:58 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 19:16:58 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 19:16:59 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/29 19:17:00 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/29 19:17:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485626579957_0036\n",
      "17/01/29 19:17:01 INFO impl.YarnClientImpl: Submitted application application_1485626579957_0036\n",
      "17/01/29 19:17:01 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1485626579957_0036/\n",
      "17/01/29 19:17:01 INFO mapreduce.Job: Running job: job_1485626579957_0036\n",
      "17/01/29 19:17:07 INFO mapreduce.Job: Job job_1485626579957_0036 running in uber mode : false\n",
      "17/01/29 19:17:07 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/29 19:17:15 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/29 19:17:23 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/29 19:17:23 INFO mapreduce.Job: Job job_1485626579957_0036 completed successfully\n",
      "17/01/29 19:17:23 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4950764\n",
      "\t\tFILE: Number of bytes written=10261788\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462817\n",
      "\t\tHDFS: Number of bytes written=368635\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11452\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5259\n",
      "\t\tTotal time spent by all map tasks (ms)=11452\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5259\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11452\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5259\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11726848\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5385216\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380826\n",
      "\t\tMap output bytes=4189106\n",
      "\t\tMap output materialized bytes=4950770\n",
      "\t\tInput split bytes=204\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12593\n",
      "\t\tReduce shuffle bytes=4950770\n",
      "\t\tReduce input records=380826\n",
      "\t\tReduce output records=12592\n",
      "\t\tSpilled Records=761652\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=144\n",
      "\t\tCPU time spent (ms)=5240\n",
      "\t\tPhysical memory (bytes) snapshot=732168192\n",
      "\t\tVirtual memory (bytes) snapshot=4668575744\n",
      "\t\tTotal committed heap usage (bytes)=736100352\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=368635\n",
      "17/01/29 19:17:23 INFO streaming.StreamJob: Output directory: HW3.3/results\n",
      "rm: `HW3.3sorted/results': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob3545238123899080251.jar tmpDir=null\n",
      "17/01/29 19:17:36 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 19:17:36 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 19:17:37 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/29 19:17:38 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/29 19:17:38 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485626579957_0037\n",
      "17/01/29 19:17:38 INFO impl.YarnClientImpl: Submitted application application_1485626579957_0037\n",
      "17/01/29 19:17:39 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1485626579957_0037/\n",
      "17/01/29 19:17:39 INFO mapreduce.Job: Running job: job_1485626579957_0037\n",
      "17/01/29 19:17:46 INFO mapreduce.Job: Job job_1485626579957_0037 running in uber mode : false\n",
      "17/01/29 19:17:46 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/29 19:17:53 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/01/29 19:17:54 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/29 19:18:00 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/29 19:18:01 INFO mapreduce.Job: Job job_1485626579957_0037 completed successfully\n",
      "17/01/29 19:18:01 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=393825\n",
      "\t\tFILE: Number of bytes written=1145165\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=372963\n",
      "\t\tHDFS: Number of bytes written=368635\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10345\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4568\n",
      "\t\tTotal time spent by all map tasks (ms)=10345\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4568\n",
      "\t\tTotal vcore-seconds taken by all map tasks=10345\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4568\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=10593280\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4677632\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12592\n",
      "\t\tMap output records=12592\n",
      "\t\tMap output bytes=368635\n",
      "\t\tMap output materialized bytes=393831\n",
      "\t\tInput split bytes=232\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12592\n",
      "\t\tReduce shuffle bytes=393831\n",
      "\t\tReduce input records=12592\n",
      "\t\tReduce output records=12592\n",
      "\t\tSpilled Records=25184\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=155\n",
      "\t\tCPU time spent (ms)=3660\n",
      "\t\tPhysical memory (bytes) snapshot=769474560\n",
      "\t\tVirtual memory (bytes) snapshot=4696821760\n",
      "\t\tTotal committed heap usage (bytes)=737673216\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=372731\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=368635\n",
      "17/01/29 19:18:01 INFO streaming.StreamJob: Output directory: HW3.3sorted/results\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "\n",
    "### First MR job to sort the products alphabetically\n",
    "# Note: only one reducer so one partition thus no need for extra coding for total sort\n",
    "\n",
    "!hdfs dfs -rm cart.txt # Comment when running first time\n",
    "!hdfs dfs -copyFromLocal cart.txt\n",
    "!hdfs dfs -rm -r HW3.3/results # Comment only when running first time\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "  -D mapreduce.job.name=\"Product frequencies and relative frequencies sorted by product\" \\\n",
    "  -files frequencies_mapper3.3.py,frequencies_reducer3.3.py \\\n",
    "  -mapper frequencies_mapper3.3.py \\\n",
    "  -reducer frequencies_reducer3.3.py \\\n",
    "  -input cart.txt \\\n",
    "  -output HW3.3/results \\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "!hdfs dfs -cat HW3.3/results/part-0000* > 33_non_sorted_freq.txt\n",
    "\n",
    "\n",
    "### Second MR job to sort the products by frequencies (and by product alphabetically if ties)\n",
    "\n",
    "#!hdfs dfs -rm 33_non_sorted_freq.txt # Comment when running first time\n",
    "!hdfs dfs -copyFromLocal 33_non_sorted_freq.txt\n",
    "!hdfs dfs -rm -r HW3.3sorted/results # Comment only when running first time\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "  -D mapreduce.job.name=\"Product sort by frequencies\" \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D stream.map.output.key.field.separator=\"\\t\" \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input 33_non_sorted_freq.txt \\\n",
    "  -output HW3.3sorted/results \\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "!hdfs dfs -cat HW3.3sorted/results/part-0000* > 33_sorted_freq.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 50 products with highest count with frequency and relative frequency:\n",
      "DAI62779\t6667\t0.0175067747831\r\n",
      "FRO40251\t3881\t0.010191059387\r\n",
      "ELE17451\t3875\t0.0101753040775\r\n",
      "GRO73461\t3602\t0.00945843749344\r\n",
      "SNA80324\t3044\t0.00799319370628\r\n",
      "ELE32164\t2851\t0.0074863979161\r\n",
      "DAI75645\t2736\t0.00718442114993\r\n",
      "SNA45677\t2455\t0.0064465474865\r\n",
      "FRO31317\t2330\t0.0061183118711\r\n",
      "DAI85309\t2293\t0.00602115412894\r\n",
      "ELE26917\t2292\t0.00601852824402\r\n",
      "FRO80039\t2233\t0.00586360103355\r\n",
      "GRO21487\t2115\t0.00555374661261\r\n",
      "SNA99873\t2083\t0.00546971829507\r\n",
      "GRO59710\t2004\t0.00526227338613\r\n",
      "GRO71621\t1920\t0.00504169905258\r\n",
      "FRO85978\t1918\t0.00503644728273\r\n",
      "GRO30386\t1840\t0.00483162825872\r\n",
      "ELE74009\t1816\t0.00476860702057\r\n",
      "GRO56726\t1784\t0.00468457870302\r\n",
      "DAI63921\t1773\t0.00465569396887\r\n",
      "GRO46854\t1756\t0.00461105392517\r\n",
      "ELE66600\t1713\t0.00449814087347\r\n",
      "DAI83733\t1712\t0.00449551498855\r\n",
      "FRO32293\t1702\t0.00446925613932\r\n",
      "ELE66810\t1697\t0.0044561267147\r\n",
      "SNA55762\t1646\t0.00432220658362\r\n",
      "DAI22177\t1627\t0.00427231477008\r\n",
      "FRO78087\t1531\t0.00402022981745\r\n",
      "ELE99737\t1516\t0.0039808415436\r\n",
      "ELE34057\t1489\t0.00390994265067\r\n",
      "GRO94758\t1489\t0.00390994265067\r\n",
      "FRO35904\t1436\t0.00377077074974\r\n",
      "FRO53271\t1420\t0.00372875659097\r\n",
      "SNA93860\t1407\t0.00369462008697\r\n",
      "SNA90094\t1390\t0.00364998004327\r\n",
      "GRO38814\t1352\t0.00355019641619\r\n",
      "ELE56788\t1345\t0.00353181522173\r\n",
      "GRO61133\t1321\t0.00346879398357\r\n",
      "DAI88807\t1316\t0.00345566455896\r\n",
      "ELE74482\t1316\t0.00345566455896\r\n",
      "ELE59935\t1311\t0.00344253513434\r\n",
      "SNA96271\t1295\t0.00340052097557\r\n",
      "DAI43223\t1290\t0.00338739155095\r\n",
      "ELE91337\t1289\t0.00338476566603\r\n",
      "GRO15017\t1275\t0.0033480032771\r\n",
      "DAI31081\t1261\t0.00331124088818\r\n",
      "GRO81087\t1220\t0.00320357960633\r\n",
      "DAI22896\t1219\t0.0032009537214\r\n",
      "GRO85051\t1214\t0.00318782429679\r\n"
     ]
    }
   ],
   "source": [
    "print \"The top 50 products with highest count with frequency and relative frequency:\"\n",
    "!head -50 33sorted_freq.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END STUDENT CODE HW33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.3.1\"></a>\n",
    "## HW3.3.1 OPTIONAL \n",
    "Using 2 reducers:  Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### START STUDENT CODE HW331 (INSERT CELLS BELOW AS NEEDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing unique_mapper3.3.1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile unique_mapper3.3.1.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "    \n",
    "# To count how many times mapper is called\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    products = line.split()\n",
    "    for product in products:\n",
    "        if product[0] <= \"m\":\n",
    "            group = \"A\"\n",
    "        else:\n",
    "            group = \"B\"\n",
    "        print '%s\\t%s\\t%s' % (group, product, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing unique_reducer3.3.1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile unique_reducer3.3.1.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "# To count how many times reducer is called\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "current_product = None\n",
    "total_unique_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    group, product, count = line.split('\\t', 2)\n",
    "\n",
    "    if current_product == product:\n",
    "        continue\n",
    "    else:\n",
    "        if current_product:\n",
    "            total_unique_count += 1  \n",
    "        current_product = product\n",
    "\n",
    "# Output the last product if needed\n",
    "if current_product == product:\n",
    "    total_unique_count += 1 \n",
    "    \n",
    "print '%s\\t%s' % (\"Number of unique products\", total_unique_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x unique_mapper3.3.1.py\n",
    "!chmod a+x unique_reducer3.3.1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted cart.txt\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob8119649920354346256.jar tmpDir=null\n",
      "17/01/30 18:23:38 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/30 18:23:38 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/30 18:23:39 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/30 18:23:39 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/30 18:23:40 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485626579957_0077\n",
      "17/01/30 18:23:40 INFO impl.YarnClientImpl: Submitted application application_1485626579957_0077\n",
      "17/01/30 18:23:40 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1485626579957_0077/\n",
      "17/01/30 18:23:40 INFO mapreduce.Job: Running job: job_1485626579957_0077\n",
      "17/01/30 18:23:46 INFO mapreduce.Job: Job job_1485626579957_0077 running in uber mode : false\n",
      "17/01/30 18:23:46 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/30 18:23:56 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/01/30 18:23:57 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/30 18:24:06 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/30 18:24:07 INFO mapreduce.Job: Job job_1485626579957_0077 completed successfully\n",
      "17/01/30 18:24:07 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5712372\n",
      "\t\tFILE: Number of bytes written=11908074\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462817\n",
      "\t\tHDFS: Number of bytes written=64\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16029\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=15812\n",
      "\t\tTotal time spent by all map tasks (ms)=16029\n",
      "\t\tTotal time spent by all reduce tasks (ms)=15812\n",
      "\t\tTotal vcore-seconds taken by all map tasks=16029\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=15812\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=16413696\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=16191488\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380824\n",
      "\t\tMap output bytes=4950712\n",
      "\t\tMap output materialized bytes=5712384\n",
      "\t\tInput split bytes=204\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12592\n",
      "\t\tReduce shuffle bytes=5712384\n",
      "\t\tReduce input records=380824\n",
      "\t\tReduce output records=2\n",
      "\t\tSpilled Records=761648\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=396\n",
      "\t\tCPU time spent (ms)=7160\n",
      "\t\tPhysical memory (bytes) snapshot=1009123328\n",
      "\t\tVirtual memory (bytes) snapshot=6244519936\n",
      "\t\tTotal committed heap usage (bytes)=1110966272\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=64\n",
      "17/01/30 18:24:07 INFO streaming.StreamJob: Output directory: HW3.3.1unique/results\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "\n",
    "### MR job to count number of unique products\n",
    "# Note: 2 reducers so need for extra coding for total sort\n",
    "\n",
    "!hdfs dfs -rm cart.txt # Comment when running first time\n",
    "!hdfs dfs -copyFromLocal cart.txt\n",
    "#!hdfs dfs -rm -r HW3.31unique/results # Comment only when running first time\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "  -D mapreduce.job.name=\"Count of unique products\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D stream.num.reduce.output.key.fields=2 \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2\" \\\n",
    "  -files unique_mapper3.3.1.py,unique_reducer3.3.1.py \\\n",
    "  -mapper unique_mapper3.3.1.py \\\n",
    "  -reducer unique_reducer3.3.1.py \\\n",
    "  -input cart.txt \\\n",
    "  -output HW3.3.1unique/results \\\n",
    "  -numReduceTasks 2\n",
    "\n",
    "!hdfs dfs -cat HW3.3.1unique/results/part-0000* > 331_unique.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique products\t6308\t\r\n",
      "Number of unique products\t6284\t\r\n"
     ]
    }
   ],
   "source": [
    "!head 331_unique.txt\n",
    "# 2 partitions from 2 reducers - thus need to sum the 2 results: 6308 + 6284 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END STUDENT CODE HW331"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.4\"></a>\n",
    "## HW3.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    "List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2. \n",
    "\n",
    "Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.  \n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2): \n",
    "\n",
    "      item1, item2, support count, support\n",
    "\n",
    "\n",
    "\n",
    "Fix the ordering of the pairs lexicographically (left to right), \n",
    "and break ties in support (between pairs, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. \n",
    "\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### START STUDENT CODE HW34 (INSERT CELLS BELOW AS NEEDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper3.4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper3.4.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "# To count how many times mapper is called\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "total_baskets = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    products = sorted(line.split())  #sorted to get same order of products in a pair\n",
    "    total_baskets += 1\n",
    "    \n",
    "    for i in range(len(products)):\n",
    "        for j in range(i+1, len(products)):\n",
    "            print '%s\\t%s' % (products[i]+','+products[j], 1)\n",
    "\n",
    "print '%s\\t%s' % (\"** baskets\", total_baskets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer3.4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer3.4.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "# To count how many times mapper is called\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "current_pair = None\n",
    "current_count = 0\n",
    "total_baskets = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    pair, count = line.split(\"\\t\")\n",
    "    \n",
    "    if pair == \"** baskets\":\n",
    "        total_baskets += int(count)\n",
    "   \n",
    "    elif current_pair == pair:\n",
    "        current_count += int(count)\n",
    "    else:\n",
    "        if current_pair:\n",
    "            support = float(current_count)/float(total_baskets)\n",
    "            if current_count >= 100:\n",
    "                print '%s\\t%s\\t%s\\t%s' % (current_pair.split(\",\")[0], current_pair.split(\",\")[1], \\\n",
    "                                      current_count, \"{0:.5f}\".format(support))\n",
    "        current_count = int(count)\n",
    "        current_pair = pair\n",
    "\n",
    "\n",
    "support = float(current_count)/float(total_baskets)\n",
    "if current_count >= 100:\n",
    "    print '%s\\t%s\\t%s\\t%s' % (current_pair.split(\",\")[0], current_pair.split(\",\")[1], \\\n",
    "                          current_count, \"{0:.5f}\".format(support))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper3.4.py\n",
    "!chmod a+x reducer3.4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted cart.txt\n",
      "Deleted HW3.4/results\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob1375595930445120434.jar tmpDir=null\n",
      "17/01/30 18:37:39 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/30 18:37:40 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/30 18:37:40 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/30 18:37:40 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/30 18:37:40 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485626579957_0078\n",
      "17/01/30 18:37:41 INFO impl.YarnClientImpl: Submitted application application_1485626579957_0078\n",
      "17/01/30 18:37:41 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1485626579957_0078/\n",
      "17/01/30 18:37:41 INFO mapreduce.Job: Running job: job_1485626579957_0078\n",
      "17/01/30 18:37:48 INFO mapreduce.Job: Job job_1485626579957_0078 running in uber mode : false\n",
      "17/01/30 18:37:48 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/30 18:37:59 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/01/30 18:38:00 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/30 18:38:11 INFO mapreduce.Job:  map 100% reduce 91%\n",
      "17/01/30 18:38:13 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/30 18:38:13 INFO mapreduce.Job: Job job_1485626579957_0078 completed successfully\n",
      "17/01/30 18:38:13 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=55749298\n",
      "\t\tFILE: Number of bytes written=111858337\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462817\n",
      "\t\tHDFS: Number of bytes written=40027\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18553\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=10902\n",
      "\t\tTotal time spent by all map tasks (ms)=18553\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10902\n",
      "\t\tTotal vcore-seconds taken by all map tasks=18553\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=10902\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=18998272\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=11163648\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=2534059\n",
      "\t\tMap output bytes=50681174\n",
      "\t\tMap output materialized bytes=55749304\n",
      "\t\tInput split bytes=204\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=877099\n",
      "\t\tReduce shuffle bytes=55749304\n",
      "\t\tReduce input records=2534059\n",
      "\t\tReduce output records=1334\n",
      "\t\tSpilled Records=5068118\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=194\n",
      "\t\tCPU time spent (ms)=14120\n",
      "\t\tPhysical memory (bytes) snapshot=931008512\n",
      "\t\tVirtual memory (bytes) snapshot=4682858496\n",
      "\t\tTotal committed heap usage (bytes)=863502336\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=40027\n",
      "17/01/30 18:38:13 INFO streaming.StreamJob: Output directory: HW3.4/results\n",
      "Deleted 34_pairs.txt\n",
      "Deleted HW3.4sorted/results\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob6235522022557018026.jar tmpDir=null\n",
      "17/01/30 18:38:27 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/30 18:38:28 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/30 18:38:28 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/30 18:38:29 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/30 18:38:29 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485626579957_0079\n",
      "17/01/30 18:38:30 INFO impl.YarnClientImpl: Submitted application application_1485626579957_0079\n",
      "17/01/30 18:38:30 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1485626579957_0079/\n",
      "17/01/30 18:38:30 INFO mapreduce.Job: Running job: job_1485626579957_0079\n",
      "17/01/30 18:38:37 INFO mapreduce.Job: Job job_1485626579957_0079 running in uber mode : false\n",
      "17/01/30 18:38:37 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/30 18:38:45 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/30 18:38:51 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/30 18:38:52 INFO mapreduce.Job: Job job_1485626579957_0079 completed successfully\n",
      "17/01/30 18:38:52 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=42701\n",
      "\t\tFILE: Number of bytes written=442788\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=44335\n",
      "\t\tHDFS: Number of bytes written=40027\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12598\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3782\n",
      "\t\tTotal time spent by all map tasks (ms)=12598\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3782\n",
      "\t\tTotal vcore-seconds taken by all map tasks=12598\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3782\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=12900352\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3872768\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1334\n",
      "\t\tMap output records=1334\n",
      "\t\tMap output bytes=40027\n",
      "\t\tMap output materialized bytes=42707\n",
      "\t\tInput split bytes=212\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1334\n",
      "\t\tReduce shuffle bytes=42707\n",
      "\t\tReduce input records=1334\n",
      "\t\tReduce output records=1334\n",
      "\t\tSpilled Records=2668\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=109\n",
      "\t\tCPU time spent (ms)=2390\n",
      "\t\tPhysical memory (bytes) snapshot=756785152\n",
      "\t\tVirtual memory (bytes) snapshot=4689903616\n",
      "\t\tTotal committed heap usage (bytes)=599261184\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=44123\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=40027\n",
      "17/01/30 18:38:52 INFO streaming.StreamJob: Output directory: HW3.4sorted/results\n",
      "total run time: 84.08 seconds\n"
     ]
    }
   ],
   "source": [
    "#Hadoop command\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "### First MR job to sort the pairs alphabetically\n",
    "\n",
    "!hdfs dfs -rm cart.txt # Comment when running first time\n",
    "!hdfs dfs -copyFromLocal cart.txt\n",
    "!hdfs dfs -rm -r HW3.4/results # Comment only when running first time\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "  -D mapreduce.job.name=\"Pair sort\" \\\n",
    "  -files mapper3.4.py,reducer3.4.py \\\n",
    "  -mapper mapper3.4.py \\\n",
    "  -reducer reducer3.4.py \\\n",
    "  -input cart.txt \\\n",
    "  -output HW3.4/results \\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "!hdfs dfs -cat HW3.4/results/part-0000* > 34_pairs.txt\n",
    "\n",
    "\n",
    "### Second MR job to sort by the pairs by frequencies and if ties by items (left to right)\n",
    "\n",
    "!hdfs dfs -rm 34_pairs.txt # Comment when running first time\n",
    "!hdfs dfs -copyFromLocal 34_pairs.txt\n",
    "!hdfs dfs -rm -r HW3.4sorted/results # Comment only when running first time\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "  -D mapreduce.job.name=\"Pair sort by frequencies\" \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D stream.num.map.output.key.fields=3 \\\n",
    "  -D stream.map.output.key.field.separator=\"\\t\" \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k3,3nr -k1,1 -k2,2\" \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input 34_pairs.txt \\\n",
    "  -output HW3.4sorted/results \\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "stop_time = time.time()\n",
    "print 'total run time: %0.2f seconds' %(stop_time - start_time)\n",
    "\n",
    "!hdfs dfs -cat HW3.4sorted/results/part-0000* > 34_pairs_sorted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 50 product pairs with corresponding support count (frequency) and support (relative frequency)\n",
      "DAI62779\tELE17451\t1592\t0.05119\r\n",
      "FRO40251\tSNA80324\t1412\t0.04540\r\n",
      "DAI75645\tFRO40251\t1254\t0.04032\r\n",
      "FRO40251\tGRO85051\t1213\t0.03900\r\n",
      "DAI62779\tGRO73461\t1139\t0.03662\r\n",
      "DAI75645\tSNA80324\t1130\t0.03633\r\n",
      "DAI62779\tFRO40251\t1070\t0.03440\r\n",
      "DAI62779\tSNA80324\t923\t0.02968\r\n",
      "DAI62779\tDAI85309\t918\t0.02952\r\n",
      "ELE32164\tGRO59710\t911\t0.02929\r\n",
      "DAI62779\tDAI75645\t882\t0.02836\r\n",
      "FRO40251\tGRO73461\t882\t0.02836\r\n",
      "DAI62779\tELE92920\t877\t0.02820\r\n",
      "FRO40251\tFRO92469\t835\t0.02685\r\n",
      "DAI62779\tELE32164\t832\t0.02675\r\n",
      "DAI75645\tGRO73461\t712\t0.02289\r\n",
      "DAI43223\tELE32164\t711\t0.02286\r\n",
      "DAI62779\tGRO30386\t709\t0.02280\r\n",
      "ELE17451\tFRO40251\t697\t0.02241\r\n",
      "DAI85309\tELE99737\t659\t0.02119\r\n",
      "DAI62779\tELE26917\t650\t0.02090\r\n",
      "GRO21487\tGRO73461\t631\t0.02029\r\n",
      "DAI62779\tSNA45677\t604\t0.01942\r\n",
      "ELE17451\tSNA80324\t597\t0.01920\r\n",
      "DAI62779\tGRO71621\t595\t0.01913\r\n",
      "DAI62779\tSNA55762\t593\t0.01907\r\n",
      "DAI62779\tDAI83733\t586\t0.01884\r\n",
      "ELE17451\tGRO73461\t580\t0.01865\r\n",
      "GRO73461\tSNA80324\t562\t0.01807\r\n",
      "DAI62779\tGRO59710\t561\t0.01804\r\n",
      "DAI62779\tFRO80039\t550\t0.01768\r\n",
      "DAI75645\tELE17451\t547\t0.01759\r\n",
      "DAI62779\tSNA93860\t537\t0.01727\r\n",
      "DAI55148\tDAI62779\t526\t0.01691\r\n",
      "DAI43223\tGRO59710\t512\t0.01646\r\n",
      "ELE17451\tELE32164\t511\t0.01643\r\n",
      "DAI62779\tSNA18336\t506\t0.01627\r\n",
      "ELE32164\tGRO73461\t486\t0.01563\r\n",
      "DAI62779\tFRO78087\t482\t0.01550\r\n",
      "DAI85309\tELE17451\t482\t0.01550\r\n",
      "DAI62779\tGRO94758\t479\t0.01540\r\n",
      "DAI62779\tGRO21487\t471\t0.01514\r\n",
      "GRO85051\tSNA80324\t471\t0.01514\r\n",
      "ELE17451\tGRO30386\t468\t0.01505\r\n",
      "FRO85978\tSNA95666\t463\t0.01489\r\n",
      "DAI62779\tFRO19221\t462\t0.01485\r\n",
      "DAI62779\tGRO46854\t461\t0.01482\r\n",
      "DAI43223\tDAI62779\t459\t0.01476\r\n",
      "ELE92920\tSNA18336\t455\t0.01463\r\n",
      "DAI88079\tFRO40251\t446\t0.01434\r\n"
     ]
    }
   ],
   "source": [
    "print \"The top 50 product pairs with corresponding support count (frequency) and support (relative frequency)\"\n",
    "!head -50 34_pairs_sorted.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation setup is as follow:  \n",
    "- single computer  \n",
    "- quad-core  \n",
    "- 2 consecutive MR jobs:  \n",
    "    * First one to sort the pairs by items pairs (alphabetically)  \n",
    "    * Second one to sort the pairs by support count  \n",
    "- 2 mappers in 1st Map reduce job and 2 mappers in the 2nd Map reduce job for a total of 4 mappers  \n",
    "- 1 reducer in 1st Map reduce job and 1 reducer in the 2nd Map reduce job for a total of 2 reducers  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first MR job (sort pairs based on the items), user defined Counters have been defined in the mapper and reducer to count how many times each is called. \n",
    "Please see report from screenshot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<H1> User-defined Counters for Mappers and Reducers in First MR Job- Job Tracker UI </H1>\n",
       "<img src=\"counters34.png\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<H1> User-defined Counters for Mappers and Reducers in First MR Job- Job Tracker UI </H1>\n",
    "<img src=\"counters34.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second MR job (re-sort the outputs from first MR job by support count), identity mapper and reducer were used. The counters are shown in the notebook when streaming under the section \"Job Counters\": there are 2 mappers and 1 reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END STUDENT CODE HW34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.5\"></a>\n",
    "## HW3.5: Stripes\n",
    "Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.\n",
    "\n",
    "Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### START STUDENT CODE HW35 (INSERT CELLS BELOW AS NEEDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper3.5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper3.5.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "# To count how many times mapper is called\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "basket_stripe = defaultdict(int)\n",
    "total_baskets = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    total_baskets += 1\n",
    "    \n",
    "    line = line.strip()\n",
    "    products = sorted(line.split())  #sorted to get same order of products in a pair\n",
    "    \n",
    "    for i in range(len(products)-1):\n",
    "        stripe = defaultdict(int)\n",
    "        for j in range(i+1, len(products)):\n",
    "            stripe[products[j]] += 1\n",
    "        stripe = dict(stripe)  # Converted to dictionary format so the string format can be \n",
    "                               # reconverted into a dictionary in the reducer    \n",
    "        print '%s\\t%s' % (products[i], stripe)\n",
    "\n",
    "\n",
    "basket_stripe[\"all\"] = total_baskets\n",
    "basket_stripe = dict(basket_stripe)\n",
    "\n",
    "print '%s\\t%s' % (\"** baskets\", basket_stripe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer3.5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer3.5.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "# To count how many times reducer is called\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing message...\\n\")\n",
    "\n",
    "# initiate current keys, count\n",
    "current_item = None\n",
    "current_count = {}\n",
    "total_baskets = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    item, stripe = line.split('\\t')    \n",
    "    stripe = ast.literal_eval(stripe)  # convert string input into a dictionary\n",
    "    \n",
    "    if item == '** baskets': \n",
    "        total_baskets += int(stripe[\"all\"])\n",
    "    \n",
    "    elif item == current_item:\n",
    "        for item2, count in stripe.items():\n",
    "            current_count[item2] = current_count.get(item2, 0) + int(count)\n",
    "    \n",
    "    else:\n",
    "        if current_item:\n",
    "            for item2, count in current_count.items():\n",
    "                if count >= 100:\n",
    "                    support = float(count)/float(total_baskets)\n",
    "                    print '%s\\t%s\\t%s\\t%s' % (current_item, item2, count, \"{0:.5f}\".format(support))\n",
    "        \n",
    "        current_item = item\n",
    "        current_count = stripe\n",
    "\n",
    "if current_item == item:        \n",
    "    for item2, count in current_count.items():\n",
    "        if count >= 100:\n",
    "            support = float(count)/float(total_baskets)\n",
    "            print '%s\\t%s\\t%s\\t%s' % (current_item, item2, count, \"{0:.5f}\".format(support))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod a+x mapper3.5.py\n",
    "!chmod a+x reducer3.5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted cart.txt\n",
      "Deleted HW3.5/results\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob8372009891191061172.jar tmpDir=null\n",
      "17/01/30 20:56:55 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/30 20:56:55 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/30 20:56:56 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/30 20:56:56 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/30 20:56:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485626579957_0086\n",
      "17/01/30 20:56:57 INFO impl.YarnClientImpl: Submitted application application_1485626579957_0086\n",
      "17/01/30 20:56:57 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1485626579957_0086/\n",
      "17/01/30 20:56:57 INFO mapreduce.Job: Running job: job_1485626579957_0086\n",
      "17/01/30 20:57:04 INFO mapreduce.Job: Job job_1485626579957_0086 running in uber mode : false\n",
      "17/01/30 20:57:04 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/30 20:57:19 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "17/01/30 20:57:20 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/01/30 20:57:22 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/30 20:57:32 INFO mapreduce.Job:  map 100% reduce 78%\n",
      "17/01/30 20:57:35 INFO mapreduce.Job:  map 100% reduce 85%\n",
      "17/01/30 20:57:38 INFO mapreduce.Job:  map 100% reduce 92%\n",
      "17/01/30 20:57:41 INFO mapreduce.Job:  map 100% reduce 99%\n",
      "17/01/30 20:57:43 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/30 20:57:43 INFO mapreduce.Job: Job job_1485626579957_0086 completed successfully\n",
      "17/01/30 20:57:43 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=42480520\n",
      "\t\tFILE: Number of bytes written=85320805\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462817\n",
      "\t\tHDFS: Number of bytes written=40027\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=29487\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=20326\n",
      "\t\tTotal time spent by all map tasks (ms)=29487\n",
      "\t\tTotal time spent by all reduce tasks (ms)=20326\n",
      "\t\tTotal vcore-seconds taken by all map tasks=29487\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=20326\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=30194688\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=20813824\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=349725\n",
      "\t\tMap output bytes=41642093\n",
      "\t\tMap output materialized bytes=42480526\n",
      "\t\tInput split bytes=204\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12012\n",
      "\t\tReduce shuffle bytes=42480526\n",
      "\t\tReduce input records=349725\n",
      "\t\tReduce output records=1334\n",
      "\t\tSpilled Records=699450\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=403\n",
      "\t\tCPU time spent (ms)=19580\n",
      "\t\tPhysical memory (bytes) snapshot=768606208\n",
      "\t\tVirtual memory (bytes) snapshot=4675780608\n",
      "\t\tTotal committed heap usage (bytes)=735051776\n",
      "\tMapper Counters\n",
      "\t\tCalls=2\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=40027\n",
      "17/01/30 20:57:43 INFO streaming.StreamJob: Output directory: HW3.5/results\n",
      "Deleted 35_stripes.txt\n",
      "Deleted HW3.5sorted/results\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.8.0.jar] /tmp/streamjob419630421773769349.jar tmpDir=null\n",
      "17/01/30 20:57:58 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/30 20:57:58 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/30 20:57:59 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/30 20:57:59 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/30 20:58:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485626579957_0087\n",
      "17/01/30 20:58:00 INFO impl.YarnClientImpl: Submitted application application_1485626579957_0087\n",
      "17/01/30 20:58:00 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1485626579957_0087/\n",
      "17/01/30 20:58:00 INFO mapreduce.Job: Running job: job_1485626579957_0087\n",
      "17/01/30 20:58:07 INFO mapreduce.Job: Job job_1485626579957_0087 running in uber mode : false\n",
      "17/01/30 20:58:07 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/30 20:58:15 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/01/30 20:58:16 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/30 20:58:22 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/30 20:58:23 INFO mapreduce.Job: Job job_1485626579957_0087 completed successfully\n",
      "17/01/30 20:58:23 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=42701\n",
      "\t\tFILE: Number of bytes written=442779\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=44339\n",
      "\t\tHDFS: Number of bytes written=40027\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11814\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4340\n",
      "\t\tTotal time spent by all map tasks (ms)=11814\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4340\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11814\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4340\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=12097536\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4444160\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1334\n",
      "\t\tMap output records=1334\n",
      "\t\tMap output bytes=40027\n",
      "\t\tMap output materialized bytes=42707\n",
      "\t\tInput split bytes=216\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1334\n",
      "\t\tReduce shuffle bytes=42707\n",
      "\t\tReduce input records=1334\n",
      "\t\tReduce output records=1334\n",
      "\t\tSpilled Records=2668\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=144\n",
      "\t\tCPU time spent (ms)=2360\n",
      "\t\tPhysical memory (bytes) snapshot=742158336\n",
      "\t\tVirtual memory (bytes) snapshot=4686544896\n",
      "\t\tTotal committed heap usage (bytes)=602931200\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=44123\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=40027\n",
      "17/01/30 20:58:23 INFO streaming.StreamJob: Output directory: HW3.5sorted/results\n",
      "total run time: 99.88 seconds\n"
     ]
    }
   ],
   "source": [
    "#Hadoop command\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "### First MR job to sort the first item alphabetically\n",
    "\n",
    "!hdfs dfs -rm cart.txt # Comment when running first time\n",
    "!hdfs dfs -copyFromLocal cart.txt\n",
    "!hdfs dfs -rm -r HW3.5/results # Comment only when running first time\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "  -D mapreduce.job.name=\"Sort with stripes\" \\\n",
    "  -files mapper3.5.py,reducer3.5.py \\\n",
    "  -mapper mapper3.5.py \\\n",
    "  -reducer reducer3.5.py \\\n",
    "  -input cart.txt \\\n",
    "  -output HW3.5/results \\\n",
    "  -numReduceTasks 1\n",
    "    \n",
    "!hdfs dfs -cat HW3.5/results/part-0000* > 35_stripes.txt\n",
    "\n",
    "\n",
    "### Second MR job to sort by the pairs by frequencies and if ties by items (left to right)\n",
    "\n",
    "!hdfs dfs -rm 35_stripes.txt # Comment when running first time\n",
    "!hdfs dfs -copyFromLocal 35_stripes.txt\n",
    "!hdfs dfs -rm -r HW3.5sorted/results # Comment only when running first time\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.8.0.jar \\\n",
    "  -D mapreduce.job.name=\"Sort by frequencies\" \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D stream.num.map.output.key.fields=3 \\\n",
    "  -D stream.map.output.key.field.separator=\"\\t\" \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k3,3nr -k1,1 -k2,2\" \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input 35_stripes.txt \\\n",
    "  -output HW3.5sorted/results \\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "stop_time = time.time()\n",
    "print 'total run time: %0.2f seconds' %(stop_time - start_time)\n",
    "\n",
    "!hdfs dfs -cat HW3.5sorted/results/part-0000* > 35_stripes_sorted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stripes\n",
      "The top 50 product pairs with corresponding support count (frequency) and support (relative frequency)\n",
      "DAI62779\tELE17451\t1592\t0.05119\r\n",
      "FRO40251\tSNA80324\t1412\t0.04540\r\n",
      "DAI75645\tFRO40251\t1254\t0.04032\r\n",
      "FRO40251\tGRO85051\t1213\t0.03900\r\n",
      "DAI62779\tGRO73461\t1139\t0.03662\r\n",
      "DAI75645\tSNA80324\t1130\t0.03633\r\n",
      "DAI62779\tFRO40251\t1070\t0.03440\r\n",
      "DAI62779\tSNA80324\t923\t0.02968\r\n",
      "DAI62779\tDAI85309\t918\t0.02952\r\n",
      "ELE32164\tGRO59710\t911\t0.02929\r\n",
      "DAI62779\tDAI75645\t882\t0.02836\r\n",
      "FRO40251\tGRO73461\t882\t0.02836\r\n",
      "DAI62779\tELE92920\t877\t0.02820\r\n",
      "FRO40251\tFRO92469\t835\t0.02685\r\n",
      "DAI62779\tELE32164\t832\t0.02675\r\n",
      "DAI75645\tGRO73461\t712\t0.02289\r\n",
      "DAI43223\tELE32164\t711\t0.02286\r\n",
      "DAI62779\tGRO30386\t709\t0.02280\r\n",
      "ELE17451\tFRO40251\t697\t0.02241\r\n",
      "DAI85309\tELE99737\t659\t0.02119\r\n",
      "DAI62779\tELE26917\t650\t0.02090\r\n",
      "GRO21487\tGRO73461\t631\t0.02029\r\n",
      "DAI62779\tSNA45677\t604\t0.01942\r\n",
      "ELE17451\tSNA80324\t597\t0.01920\r\n",
      "DAI62779\tGRO71621\t595\t0.01913\r\n",
      "DAI62779\tSNA55762\t593\t0.01907\r\n",
      "DAI62779\tDAI83733\t586\t0.01884\r\n",
      "ELE17451\tGRO73461\t580\t0.01865\r\n",
      "GRO73461\tSNA80324\t562\t0.01807\r\n",
      "DAI62779\tGRO59710\t561\t0.01804\r\n",
      "DAI62779\tFRO80039\t550\t0.01768\r\n",
      "DAI75645\tELE17451\t547\t0.01759\r\n",
      "DAI62779\tSNA93860\t537\t0.01727\r\n",
      "DAI55148\tDAI62779\t526\t0.01691\r\n",
      "DAI43223\tGRO59710\t512\t0.01646\r\n",
      "ELE17451\tELE32164\t511\t0.01643\r\n",
      "DAI62779\tSNA18336\t506\t0.01627\r\n",
      "ELE32164\tGRO73461\t486\t0.01563\r\n",
      "DAI62779\tFRO78087\t482\t0.01550\r\n",
      "DAI85309\tELE17451\t482\t0.01550\r\n",
      "DAI62779\tGRO94758\t479\t0.01540\r\n",
      "DAI62779\tGRO21487\t471\t0.01514\r\n",
      "GRO85051\tSNA80324\t471\t0.01514\r\n",
      "ELE17451\tGRO30386\t468\t0.01505\r\n",
      "FRO85978\tSNA95666\t463\t0.01489\r\n",
      "DAI62779\tFRO19221\t462\t0.01485\r\n",
      "DAI62779\tGRO46854\t461\t0.01482\r\n",
      "DAI43223\tDAI62779\t459\t0.01476\r\n",
      "ELE92920\tSNA18336\t455\t0.01463\r\n",
      "DAI88079\tFRO40251\t446\t0.01434\r\n"
     ]
    }
   ],
   "source": [
    "print \"Stripes\"\n",
    "print \"The top 50 product pairs with corresponding support count (frequency) and support (relative frequency)\"\n",
    "!head -50 35_stripes_sorted.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation setup is as follow:  \n",
    "- single computer  \n",
    "- quad-core  \n",
    "- 2 consecutive MR jobs:  \n",
    "    * First one to sort the outputs of key item and stripes by key items (alphabetically)  \n",
    "    * Second one to sort the item pairs by support count  \n",
    "- 2 mappers in 1st Map reduce job and 2 mappers in the 2nd Map reduce job for a total of 4 mappers  \n",
    "- 1 reducer in 1st Map reduce job and 1 reducer in the 2nd Map reduce job for a total of 2 reducers  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first MR job (sort pairs based on the items), user defined Counters have been defined in the mapper and reducer to count how many times each is called. \n",
    "Please see report from screenshot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<H1> User-defined Counters for Mappers and Reducers in First MR Job- Job Tracker UI </H1>\n",
       "<img src=\"counters35.png\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<H1> User-defined Counters for Mappers and Reducers in First MR Job- Job Tracker UI </H1>\n",
    "<img src=\"counters35.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second MR job (re-sort the outputs from first MR job by support count), identity mapper and reducer were used. The counters are shown in the notebook when streaming under the section \"Job Counters\": there are 2 mappers and 1 reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For implementing pairs and stripes, the framework for both cases is similar: one map reduce job with 2 mappers and 1 reducer for sorting the outputs by item name alphabetically and a second map reduce job with again 2 mappers and 1 reducer for sorting the outputs by frequency.\n",
    "\n",
    "I was expecting a faster overall map reduce job for the stripes implementation since less data were to be transferred over the network from mapper to reducer. However the stripes(100 secs) tooks 15 seconds longer than the pairs (84 secs). The conversion of the string from reducer inputs into a dictionary using ast.literal.eval has added processing time in the stripes reducer."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
